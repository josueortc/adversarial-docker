{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from cifar10_models import return_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'resnet'\n",
    "# New - Josue wants low/high alpha test. \n",
    "name = 'simplea3' # was simplehigha\n",
    "name = 'simpleapoint33' # was simplelowa\n",
    "name = 'simplea10'\n",
    "name = 'simpleapoint1'\n",
    "# New - Train Linear Models\n",
    "name = 'linear' # actually linear\n",
    "name = 'simplelinear' # conv linear\n",
    "# New - Train nonlinear models\n",
    "name = 'fclinear' # FC relu\n",
    "name = 'lc' # LC Relu\n",
    "name = 'circular' # FC Conv\n",
    "name = 'simple' # LC Conv\n",
    "# New - Ankit wants simplelinear with a = .1, 10\n",
    "name = 'simplelineara10'\n",
    "name = 'simplelinearapoint1'\n",
    "# And now also for lc\n",
    "name = 'lca10'\n",
    "name = 'lcapoint1'\n",
    "# And finally, lclinear\n",
    "name = 'lclinear'\n",
    "name = 'lclineara10'\n",
    "name = 'lclinearapoint1'\n",
    "# Add new circularlinear (which is the NONlinear circular conv)\n",
    "name = 'circularlinear'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model (Saving every 30 iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      "0 391 Loss: 2.294 | Acc: 11.719% (15/128)\n",
      "100 391 Loss: 2.055 | Acc: 25.201% (3258/12928)\n",
      "200 391 Loss: 1.960 | Acc: 30.212% (7773/25728)\n",
      "300 391 Loss: 1.906 | Acc: 32.641% (12576/38528)\n",
      "99 100 Loss: 1.738 | Acc: 39.560% (3956/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 391 Loss: 1.745 | Acc: 44.531% (57/128)\n",
      "100 391 Loss: 1.712 | Acc: 40.524% (5239/12928)\n",
      "200 391 Loss: 1.707 | Acc: 40.948% (10535/25728)\n",
      "300 391 Loss: 1.693 | Acc: 41.380% (15943/38528)\n",
      "99 100 Loss: 1.631 | Acc: 43.350% (4335/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "0 391 Loss: 1.617 | Acc: 44.531% (57/128)\n",
      "100 391 Loss: 1.613 | Acc: 45.204% (5844/12928)\n",
      "200 391 Loss: 1.594 | Acc: 45.398% (11680/25728)\n",
      "300 391 Loss: 1.581 | Acc: 45.590% (17565/38528)\n",
      "99 100 Loss: 1.520 | Acc: 46.430% (4643/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 391 Loss: 1.433 | Acc: 49.219% (63/128)\n",
      "100 391 Loss: 1.482 | Acc: 48.886% (6320/12928)\n",
      "200 391 Loss: 1.476 | Acc: 49.153% (12646/25728)\n",
      "300 391 Loss: 1.470 | Acc: 49.221% (18964/38528)\n",
      "99 100 Loss: 1.422 | Acc: 50.440% (5044/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 391 Loss: 1.341 | Acc: 52.344% (67/128)\n",
      "100 391 Loss: 1.392 | Acc: 51.980% (6720/12928)\n",
      "200 391 Loss: 1.384 | Acc: 52.181% (13425/25728)\n",
      "300 391 Loss: 1.380 | Acc: 52.544% (20244/38528)\n",
      "99 100 Loss: 1.391 | Acc: 51.310% (5131/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      "0 391 Loss: 1.359 | Acc: 50.781% (65/128)\n",
      "100 391 Loss: 1.362 | Acc: 53.195% (6877/12928)\n",
      "200 391 Loss: 1.359 | Acc: 53.160% (13677/25728)\n",
      "300 391 Loss: 1.355 | Acc: 53.244% (20514/38528)\n",
      "99 100 Loss: 1.375 | Acc: 51.950% (5195/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      "0 391 Loss: 1.375 | Acc: 54.688% (70/128)\n",
      "100 391 Loss: 1.334 | Acc: 53.991% (6980/12928)\n",
      "200 391 Loss: 1.333 | Acc: 54.031% (13901/25728)\n",
      "300 391 Loss: 1.333 | Acc: 53.930% (20778/38528)\n",
      "99 100 Loss: 1.357 | Acc: 52.500% (5250/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "0 391 Loss: 1.336 | Acc: 57.031% (73/128)\n",
      "100 391 Loss: 1.311 | Acc: 54.486% (7044/12928)\n",
      "200 391 Loss: 1.314 | Acc: 54.489% (14019/25728)\n",
      "300 391 Loss: 1.316 | Acc: 54.516% (21004/38528)\n",
      "99 100 Loss: 1.347 | Acc: 52.300% (5230/10000)\n",
      "\n",
      "Epoch: 8\n",
      "0 391 Loss: 1.480 | Acc: 45.312% (58/128)\n",
      "100 391 Loss: 1.304 | Acc: 54.796% (7084/12928)\n",
      "200 391 Loss: 1.296 | Acc: 55.263% (14218/25728)\n",
      "300 391 Loss: 1.296 | Acc: 55.261% (21291/38528)\n",
      "99 100 Loss: 1.338 | Acc: 52.920% (5292/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "0 391 Loss: 1.302 | Acc: 53.906% (69/128)\n",
      "100 391 Loss: 1.302 | Acc: 55.005% (7111/12928)\n",
      "200 391 Loss: 1.296 | Acc: 55.667% (14322/25728)\n",
      "300 391 Loss: 1.294 | Acc: 55.365% (21331/38528)\n",
      "99 100 Loss: 1.333 | Acc: 53.270% (5327/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      "0 391 Loss: 1.075 | Acc: 63.281% (81/128)\n",
      "100 391 Loss: 1.268 | Acc: 55.979% (7237/12928)\n",
      "200 391 Loss: 1.282 | Acc: 55.791% (14354/25728)\n",
      "300 391 Loss: 1.281 | Acc: 55.752% (21480/38528)\n",
      "99 100 Loss: 1.328 | Acc: 53.410% (5341/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      "0 391 Loss: 1.202 | Acc: 56.250% (72/128)\n",
      "100 391 Loss: 1.276 | Acc: 56.041% (7245/12928)\n",
      "200 391 Loss: 1.282 | Acc: 55.943% (14393/25728)\n",
      "300 391 Loss: 1.280 | Acc: 55.783% (21492/38528)\n",
      "99 100 Loss: 1.325 | Acc: 53.510% (5351/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      "0 391 Loss: 1.347 | Acc: 48.438% (62/128)\n",
      "100 391 Loss: 1.266 | Acc: 56.258% (7273/12928)\n",
      "200 391 Loss: 1.272 | Acc: 55.997% (14407/25728)\n",
      "300 391 Loss: 1.273 | Acc: 55.959% (21560/38528)\n",
      "99 100 Loss: 1.325 | Acc: 53.430% (5343/10000)\n",
      "\n",
      "Epoch: 13\n",
      "0 391 Loss: 1.343 | Acc: 48.438% (62/128)\n",
      "100 391 Loss: 1.285 | Acc: 56.095% (7252/12928)\n",
      "200 391 Loss: 1.276 | Acc: 56.102% (14434/25728)\n",
      "300 391 Loss: 1.272 | Acc: 56.172% (21642/38528)\n",
      "99 100 Loss: 1.322 | Acc: 53.430% (5343/10000)\n",
      "\n",
      "Epoch: 14\n",
      "0 391 Loss: 1.379 | Acc: 52.344% (67/128)\n",
      "100 391 Loss: 1.272 | Acc: 56.474% (7301/12928)\n",
      "200 391 Loss: 1.282 | Acc: 56.063% (14424/25728)\n",
      "300 391 Loss: 1.277 | Acc: 56.125% (21624/38528)\n",
      "99 100 Loss: 1.322 | Acc: 53.540% (5354/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      "0 391 Loss: 1.331 | Acc: 56.250% (72/128)\n",
      "100 391 Loss: 1.264 | Acc: 56.451% (7298/12928)\n",
      "200 391 Loss: 1.267 | Acc: 56.359% (14500/25728)\n",
      "300 391 Loss: 1.269 | Acc: 56.273% (21681/38528)\n",
      "99 100 Loss: 1.320 | Acc: 53.730% (5373/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n",
      "0 391 Loss: 1.274 | Acc: 60.156% (77/128)\n",
      "100 391 Loss: 1.265 | Acc: 55.987% (7238/12928)\n",
      "200 391 Loss: 1.264 | Acc: 56.133% (14442/25728)\n",
      "300 391 Loss: 1.268 | Acc: 56.157% (21636/38528)\n",
      "99 100 Loss: 1.320 | Acc: 53.580% (5358/10000)\n",
      "\n",
      "Epoch: 17\n",
      "0 391 Loss: 1.400 | Acc: 51.562% (66/128)\n",
      "100 391 Loss: 1.263 | Acc: 56.521% (7307/12928)\n",
      "200 391 Loss: 1.263 | Acc: 56.421% (14516/25728)\n",
      "300 391 Loss: 1.269 | Acc: 56.167% (21640/38528)\n",
      "99 100 Loss: 1.319 | Acc: 53.670% (5367/10000)\n",
      "\n",
      "Epoch: 18\n",
      "0 391 Loss: 1.223 | Acc: 64.062% (82/128)\n",
      "100 391 Loss: 1.267 | Acc: 56.552% (7311/12928)\n",
      "200 391 Loss: 1.264 | Acc: 56.654% (14576/25728)\n",
      "300 391 Loss: 1.265 | Acc: 56.559% (21791/38528)\n",
      "99 100 Loss: 1.319 | Acc: 53.670% (5367/10000)\n",
      "\n",
      "Epoch: 19\n",
      "0 391 Loss: 1.266 | Acc: 53.906% (69/128)\n",
      "100 391 Loss: 1.265 | Acc: 56.018% (7242/12928)\n",
      "200 391 Loss: 1.268 | Acc: 56.219% (14464/25728)\n",
      "300 391 Loss: 1.268 | Acc: 56.260% (21676/38528)\n",
      "99 100 Loss: 1.319 | Acc: 53.610% (5361/10000)\n",
      "\n",
      "Epoch: 20\n",
      "0 391 Loss: 1.267 | Acc: 60.938% (78/128)\n",
      "100 391 Loss: 1.293 | Acc: 55.244% (7142/12928)\n",
      "200 391 Loss: 1.293 | Acc: 55.002% (14151/25728)\n",
      "300 391 Loss: 1.290 | Acc: 55.004% (21192/38528)\n",
      "99 100 Loss: 1.313 | Acc: 53.530% (5353/10000)\n",
      "\n",
      "Epoch: 21\n",
      "0 391 Loss: 1.256 | Acc: 53.125% (68/128)\n",
      "100 391 Loss: 1.262 | Acc: 56.057% (7247/12928)\n",
      "200 391 Loss: 1.258 | Acc: 56.386% (14507/25728)\n",
      "300 391 Loss: 1.258 | Acc: 56.362% (21715/38528)\n",
      "99 100 Loss: 1.311 | Acc: 53.260% (5326/10000)\n",
      "\n",
      "Epoch: 22\n",
      "0 391 Loss: 1.247 | Acc: 53.125% (68/128)\n",
      "100 391 Loss: 1.238 | Acc: 56.567% (7313/12928)\n",
      "200 391 Loss: 1.230 | Acc: 57.043% (14676/25728)\n",
      "300 391 Loss: 1.230 | Acc: 57.119% (22007/38528)\n",
      "99 100 Loss: 1.287 | Acc: 54.080% (5408/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      "0 391 Loss: 1.275 | Acc: 54.688% (70/128)\n",
      "100 391 Loss: 1.204 | Acc: 58.323% (7540/12928)\n",
      "200 391 Loss: 1.206 | Acc: 58.279% (14994/25728)\n",
      "300 391 Loss: 1.205 | Acc: 58.064% (22371/38528)\n",
      "99 100 Loss: 1.282 | Acc: 54.290% (5429/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 24\n",
      "0 391 Loss: 1.107 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.157 | Acc: 60.295% (7795/12928)\n",
      "200 391 Loss: 1.159 | Acc: 60.207% (15490/25728)\n",
      "300 391 Loss: 1.166 | Acc: 59.912% (23083/38528)\n",
      "99 100 Loss: 1.267 | Acc: 55.020% (5502/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 25\n",
      "0 391 Loss: 1.155 | Acc: 60.938% (78/128)\n",
      "100 391 Loss: 1.166 | Acc: 59.793% (7730/12928)\n",
      "200 391 Loss: 1.168 | Acc: 59.888% (15408/25728)\n",
      "300 391 Loss: 1.160 | Acc: 60.164% (23180/38528)\n",
      "99 100 Loss: 1.263 | Acc: 55.600% (5560/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 26\n",
      "0 391 Loss: 1.174 | Acc: 55.469% (71/128)\n",
      "100 391 Loss: 1.150 | Acc: 60.783% (7858/12928)\n",
      "200 391 Loss: 1.160 | Acc: 60.180% (15483/25728)\n",
      "300 391 Loss: 1.153 | Acc: 60.379% (23263/38528)\n",
      "99 100 Loss: 1.247 | Acc: 55.700% (5570/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 27\n",
      "0 391 Loss: 1.089 | Acc: 68.750% (88/128)\n",
      "100 391 Loss: 1.132 | Acc: 61.347% (7931/12928)\n",
      "200 391 Loss: 1.137 | Acc: 61.132% (15728/25728)\n",
      "300 391 Loss: 1.148 | Acc: 60.460% (23294/38528)\n",
      "99 100 Loss: 1.245 | Acc: 55.950% (5595/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 28\n",
      "0 391 Loss: 1.108 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.143 | Acc: 60.636% (7839/12928)\n",
      "200 391 Loss: 1.133 | Acc: 61.167% (15737/25728)\n",
      "300 391 Loss: 1.138 | Acc: 60.992% (23499/38528)\n",
      "99 100 Loss: 1.241 | Acc: 56.130% (5613/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 29\n",
      "0 391 Loss: 1.130 | Acc: 60.156% (77/128)\n",
      "100 391 Loss: 1.123 | Acc: 61.549% (7957/12928)\n",
      "200 391 Loss: 1.136 | Acc: 61.248% (15758/25728)\n",
      "300 391 Loss: 1.138 | Acc: 61.059% (23525/38528)\n",
      "99 100 Loss: 1.240 | Acc: 55.920% (5592/10000)\n",
      "\n",
      "Epoch: 30\n",
      "0 391 Loss: 1.088 | Acc: 63.281% (81/128)\n",
      "100 391 Loss: 1.128 | Acc: 60.945% (7879/12928)\n",
      "200 391 Loss: 1.134 | Acc: 60.965% (15685/25728)\n",
      "300 391 Loss: 1.131 | Acc: 61.132% (23553/38528)\n",
      "99 100 Loss: 1.240 | Acc: 56.490% (5649/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 31\n",
      "0 391 Loss: 1.138 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.131 | Acc: 61.564% (7959/12928)\n",
      "200 391 Loss: 1.134 | Acc: 61.346% (15783/25728)\n",
      "300 391 Loss: 1.135 | Acc: 61.228% (23590/38528)\n",
      "99 100 Loss: 1.239 | Acc: 56.290% (5629/10000)\n",
      "\n",
      "Epoch: 32\n",
      "0 391 Loss: 1.114 | Acc: 60.938% (78/128)\n",
      "100 391 Loss: 1.141 | Acc: 60.883% (7871/12928)\n",
      "200 391 Loss: 1.136 | Acc: 61.225% (15752/25728)\n",
      "300 391 Loss: 1.132 | Acc: 61.181% (23572/38528)\n",
      "99 100 Loss: 1.237 | Acc: 56.370% (5637/10000)\n",
      "\n",
      "Epoch: 33\n",
      "0 391 Loss: 0.949 | Acc: 72.656% (93/128)\n",
      "100 391 Loss: 1.141 | Acc: 60.992% (7885/12928)\n",
      "200 391 Loss: 1.126 | Acc: 61.493% (15821/25728)\n",
      "300 391 Loss: 1.132 | Acc: 61.252% (23599/38528)\n",
      "99 100 Loss: 1.238 | Acc: 56.260% (5626/10000)\n",
      "\n",
      "Epoch: 34\n",
      "0 391 Loss: 1.071 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.125 | Acc: 61.479% (7948/12928)\n",
      "200 391 Loss: 1.124 | Acc: 61.198% (15745/25728)\n",
      "300 391 Loss: 1.127 | Acc: 61.194% (23577/38528)\n",
      "99 100 Loss: 1.237 | Acc: 56.270% (5627/10000)\n",
      "\n",
      "Epoch: 35\n",
      "0 391 Loss: 1.044 | Acc: 63.281% (81/128)\n",
      "100 391 Loss: 1.126 | Acc: 61.502% (7951/12928)\n",
      "200 391 Loss: 1.125 | Acc: 61.692% (15872/25728)\n",
      "300 391 Loss: 1.131 | Acc: 61.316% (23624/38528)\n",
      "99 100 Loss: 1.237 | Acc: 56.230% (5623/10000)\n",
      "\n",
      "Epoch: 36\n",
      "0 391 Loss: 1.207 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.125 | Acc: 61.719% (7979/12928)\n",
      "200 391 Loss: 1.132 | Acc: 61.357% (15786/25728)\n",
      "300 391 Loss: 1.129 | Acc: 61.449% (23675/38528)\n",
      "99 100 Loss: 1.237 | Acc: 56.230% (5623/10000)\n",
      "\n",
      "Epoch: 37\n",
      "0 391 Loss: 1.105 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.134 | Acc: 60.907% (7874/12928)\n",
      "200 391 Loss: 1.132 | Acc: 61.194% (15744/25728)\n",
      "300 391 Loss: 1.127 | Acc: 61.311% (23622/38528)\n",
      "99 100 Loss: 1.237 | Acc: 56.310% (5631/10000)\n",
      "\n",
      "Epoch: 38\n",
      "0 391 Loss: 1.301 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.137 | Acc: 60.968% (7882/12928)\n",
      "200 391 Loss: 1.131 | Acc: 61.400% (15797/25728)\n",
      "300 391 Loss: 1.127 | Acc: 61.490% (23691/38528)\n",
      "99 100 Loss: 1.237 | Acc: 56.310% (5631/10000)\n",
      "\n",
      "Epoch: 39\n",
      "0 391 Loss: 1.116 | Acc: 61.719% (79/128)\n",
      "100 391 Loss: 1.131 | Acc: 60.938% (7878/12928)\n",
      "200 391 Loss: 1.133 | Acc: 61.081% (15715/25728)\n",
      "300 391 Loss: 1.131 | Acc: 61.179% (23571/38528)\n",
      "99 100 Loss: 1.236 | Acc: 56.280% (5628/10000)\n",
      "56.49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "#parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "#arser.add_argument('--resume', '-r', action='store_true',\n",
    "#                    help='resume from checkpoint')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# For contaminated data, Josue wants linear vs nonlinear, FC vs Conv\n",
    "# Linear FC : 'linear'\n",
    "# Nonlinear FC : 'fclinear'\n",
    "# Linear Conv : 'simplelinear'\n",
    "# Nonlinear Conv : 'simple'\n",
    "\n",
    "name = 'linear'\n",
    "name = 'fclinear'\n",
    "name = 'simplelinear'\n",
    "name = 'simple'\n",
    "\n",
    "\n",
    "args = {}\n",
    "args['resume'] = False\n",
    "args['lr'] = 0.1\n",
    "#args['lr']/=10 # high alpha requires lower learning rate - try dividing by alpha to start, dividing by 3 gives 55.4% final test acc\n",
    "# Basee lr works fine for linear, gets ~40% accuracy\n",
    "# simple linear requires lr lower, /10 works ok to ~40% acc\n",
    "args['lr']/=50\n",
    "# fclinear also does well with /10 ~ 54+%\n",
    "# lc also does well with /10, 59+%\n",
    "# circular fails at /10 and /20, try /50, gets to around 41%\n",
    "# final is simple, /50 seems to work fine, gets to around 57.5%\n",
    "# simpelinear a = 10 got to around 40%\n",
    "# '', a = .1 got to around 42%\n",
    "# lc, a = 10, /10 got to around 56%\n",
    "# lc, a = .1, /10 gets to around 54%\n",
    "# lclinear (base) /10 got to 35%\n",
    "# lclinear a = 10 /10 got to 30%\n",
    "# lclinear,a = .1 /10 got to 41%\n",
    "# circularlinear, /50 got to 39%\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = return_model(name)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if args['resume']:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args['lr'],\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "lambda1 = lambda epoch: 0.3**((epoch%20)//4)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "\n",
    "def train(epoch,name):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        #if epoch == 0 and batch_idx == 0:\n",
    "        if np.mod(batch_idx,30)==1:\n",
    "            torch.save(net.state_dict(), './cifar_resnet/ckpt_{}_{}_{}.pth'.format(name, epoch, batch_idx))\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        #if batch_idx%1 == 0:\n",
    "            #torch.save(net.state_dict(), './cifar_resnet/ckpt_track_{}_{}.pth'.format(epoch, batch_idx+1))\n",
    "        if batch_idx%100 == 0:\n",
    "            print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'%(train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch,name):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'% (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('cifar_resnet'):\n",
    "            os.mkdir('cifar_resnet')\n",
    "        torch.save(state, './cifar_resnet/ckpt_{}.pth'.format(name))\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+40):\n",
    "    train(epoch,name)\n",
    "    test(epoch,name)\n",
    "    scheduler.step()\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-poisoned: \n",
    "#L FC : 40.72, 40.75, 40.78, 40.83, 40.92\n",
    "#NL FC: 48.63, 48.38, 48.53, 48.52, 48.45\n",
    "#L Conv: 41.82, 41.87, 41.74, 41.77, 41.83\n",
    "#NL Conv: 56.67, 55.84, 57.23, 57.07, 56.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "375.19301904197795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:93: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:94: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:116: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:117: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:118: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "# New Experiment - try changing the dataset to have fourier features with uniform power at all frequencies\n",
    "# Start by trying to load and modify a single image?\n",
    "\n",
    "# ALl set up - now saves a new dataset based on 3 hyperparams epislon, p, rho\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from cifar10_models import return_model\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import random\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=100, shuffle=False, num_workers=1) # no shuffle here, changed batch to 100\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=1)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def get_fourier(imag):\n",
    "    image_sample = np.fft.fft2(imag)\n",
    "    F_orig = np.fft.fftshift(image_sample)\n",
    "    #F_orig = np.log(np.abs(F_orig))\n",
    "    return F_orig\n",
    "\n",
    "new_train = np.zeros((50000,3,32,32))\n",
    "new_train_classes = np.zeros(50000)\n",
    "# Try setting up a system to generate a (class depedent) mask\n",
    "epsilon = 2e-3 * 50 # random perturbation size - Ankit requested that it be 'comparable' to L1 norm of perturbations. These tend to be around .03, so choose .03/(32*32*p)? ~= 3e-5\n",
    "p = .15 # random perturbation density from 1 for full to e.g. .01 for very sparse\n",
    "rho = 0 # randomness applied to each individual image - e.g. making it non-deterministic\n",
    "perturbation = np.random.randn(10,3,32,32)*epsilon\n",
    "perturbation *= (np.random.rand(10,3,32,32)<p)\n",
    "# just add perturbation[class,:,:] to each (fourier) data\n",
    "\n",
    "print(np.linalg.norm(perturbation.flatten(),ord = 1))\n",
    "\n",
    "# Load image\n",
    "for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "    #print(inputs.shape) # 128 (batch) by 3 channels by 32 by 32\n",
    "    #fig = plt.figure(figsize=(8, 8))\n",
    "    #plt.imshow(inputs[0,0,:,:], vmin=None, vmax = None, interpolation='nearest')\n",
    "    #plt.show()\n",
    "    # Take fourier\n",
    "    #ftest = get_fourier(inputs[0,0,:,:].data.numpy())\n",
    "    #fig = plt.figure(figsize=(8, 8))\n",
    "    #plt.imshow(np.abs(ftest), vmin=None, vmax = None, interpolation='nearest')\n",
    "    #plt.show()\n",
    "    # Try inverse?\n",
    "    #ftest2 = np.fft.ifftshift(ftest)\n",
    "    #forig = np.fft.ifft2(ftest2)\n",
    "    #fig = plt.figure(figsize=(8, 8))\n",
    "    #plt.imshow(np.real(forig), vmin=None, vmax = None, interpolation='nearest')\n",
    "    #plt.show()\n",
    "    # Back to original (more or less)\n",
    "    \n",
    "    # Test works - on to making the change\n",
    "    for i in range(100): \n",
    "        fd1 = get_fourier(inputs[i,0,:,:].data.numpy())\n",
    "        fd2 = get_fourier(inputs[i,1,:,:].data.numpy())\n",
    "        fd3 = get_fourier(inputs[i,2,:,:].data.numpy())\n",
    "        fd1 += perturbation[targets.numpy()[i],0,:,:]\n",
    "        fd1 += np.random.randn(32,32)*rho\n",
    "        fd2 += perturbation[targets.numpy()[i],1,:,:]\n",
    "        fd2 += np.random.randn(32,32)*rho\n",
    "        fd3 += perturbation[targets.numpy()[i],2,:,:]\n",
    "        fd3 += np.random.randn(32,32)*rho\n",
    "        new_train[100*batch_idx + i,0,:,:] = np.fft.ifft2(np.fft.ifftshift(fd1))\n",
    "        new_train[100*batch_idx + i,1,:,:] = np.fft.ifft2(np.fft.ifftshift(fd2))\n",
    "        new_train[100*batch_idx + i,2,:,:] = np.fft.ifft2(np.fft.ifftshift(fd3))\n",
    "\n",
    "        new_train_classes[100*batch_idx + i] = targets.numpy()[i]\n",
    "    \n",
    "    # Ok works, now need to store the whole thing as a giant numpy array of size [samples] x [image,category]...\n",
    "#np.save('./data/modified_cifar_train.npy', new_train)\n",
    "np.savez('./data/modified_cifar_train_eps:{}_p:{}_rho:{}.npy'.format(epsilon, p, rho), images=new_train, target=new_train_classes)\n",
    "# Have to do test set with same perturbation, so do that now too\n",
    "\n",
    "new_test = np.zeros((10000,3,32,32))\n",
    "new_test_classes = np.zeros(10000)\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    for i in range(100): \n",
    "        fd1 = get_fourier(inputs[i,0,:,:].data.numpy())\n",
    "        fd2 = get_fourier(inputs[i,1,:,:].data.numpy())\n",
    "        fd3 = get_fourier(inputs[i,2,:,:].data.numpy())\n",
    "        fd1 += perturbation[targets.numpy()[i],0,:,:]\n",
    "        fd1 += np.random.randn(32,32)*rho\n",
    "        fd2 += perturbation[targets.numpy()[i],1,:,:]\n",
    "        fd2 += np.random.randn(32,32)*rho\n",
    "        fd3 += perturbation[targets.numpy()[i],2,:,:]\n",
    "        fd3 += np.random.randn(32,32)*rho\n",
    "        new_test[100*batch_idx + i,0,:,:] = np.fft.ifft2(np.fft.ifftshift(fd1))\n",
    "        new_test[100*batch_idx + i,1,:,:] = np.fft.ifft2(np.fft.ifftshift(fd2))\n",
    "        new_test[100*batch_idx + i,2,:,:] = np.fft.ifft2(np.fft.ifftshift(fd3))\n",
    "\n",
    "        new_test_classes[100*batch_idx + i] = targets.numpy()[i]\n",
    "np.savez('./data/modified_cifar_test_eps:{}_p:{}_rho:{}.npy'.format(epsilon, p, rho), images=new_test, target=new_test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      "0 391 Loss: 2.334 | Acc: 5.469% (7/128)\n",
      "100 391 Loss: 2.063 | Acc: 25.967% (3357/12928)\n",
      "200 391 Loss: 1.954 | Acc: 30.605% (7874/25728)\n",
      "300 391 Loss: 1.899 | Acc: 33.051% (12734/38528)\n",
      "99 100 Loss: 1.727 | Acc: 40.560% (4056/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 391 Loss: 1.677 | Acc: 42.188% (54/128)\n",
      "100 391 Loss: 1.699 | Acc: 41.538% (5370/12928)\n",
      "200 391 Loss: 1.686 | Acc: 42.312% (10886/25728)\n",
      "300 391 Loss: 1.670 | Acc: 42.813% (16495/38528)\n",
      "99 100 Loss: 1.576 | Acc: 45.600% (4560/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "0 391 Loss: 1.604 | Acc: 50.781% (65/128)\n",
      "100 391 Loss: 1.550 | Acc: 47.068% (6085/12928)\n",
      "200 391 Loss: 1.545 | Acc: 47.450% (12208/25728)\n",
      "300 391 Loss: 1.532 | Acc: 47.716% (18384/38528)\n",
      "99 100 Loss: 1.462 | Acc: 49.630% (4963/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 391 Loss: 1.437 | Acc: 52.344% (67/128)\n",
      "100 391 Loss: 1.446 | Acc: 50.093% (6476/12928)\n",
      "200 391 Loss: 1.434 | Acc: 50.805% (13071/25728)\n",
      "300 391 Loss: 1.425 | Acc: 51.186% (19721/38528)\n",
      "99 100 Loss: 1.422 | Acc: 50.250% (5025/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 391 Loss: 1.350 | Acc: 49.219% (63/128)\n",
      "100 391 Loss: 1.344 | Acc: 54.401% (7033/12928)\n",
      "200 391 Loss: 1.334 | Acc: 54.952% (14138/25728)\n",
      "300 391 Loss: 1.334 | Acc: 54.880% (21144/38528)\n",
      "99 100 Loss: 1.352 | Acc: 53.720% (5372/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      "0 391 Loss: 1.350 | Acc: 53.125% (68/128)\n",
      "100 391 Loss: 1.308 | Acc: 55.623% (7191/12928)\n",
      "200 391 Loss: 1.310 | Acc: 55.566% (14296/25728)\n",
      "300 391 Loss: 1.307 | Acc: 55.837% (21513/38528)\n",
      "99 100 Loss: 1.334 | Acc: 54.320% (5432/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      "0 391 Loss: 1.353 | Acc: 53.125% (68/128)\n",
      "100 391 Loss: 1.277 | Acc: 56.544% (7310/12928)\n",
      "200 391 Loss: 1.281 | Acc: 56.503% (14537/25728)\n",
      "300 391 Loss: 1.279 | Acc: 56.585% (21801/38528)\n",
      "99 100 Loss: 1.305 | Acc: 55.720% (5572/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "0 391 Loss: 1.256 | Acc: 60.938% (78/128)\n",
      "100 391 Loss: 1.270 | Acc: 56.846% (7349/12928)\n",
      "200 391 Loss: 1.271 | Acc: 56.907% (14641/25728)\n",
      "300 391 Loss: 1.263 | Acc: 57.280% (22069/38528)\n",
      "99 100 Loss: 1.292 | Acc: 55.570% (5557/10000)\n",
      "\n",
      "Epoch: 8\n",
      "0 391 Loss: 1.353 | Acc: 59.375% (76/128)\n",
      "100 391 Loss: 1.245 | Acc: 58.021% (7501/12928)\n",
      "200 391 Loss: 1.244 | Acc: 58.100% (14948/25728)\n",
      "300 391 Loss: 1.239 | Acc: 58.446% (22518/38528)\n",
      "99 100 Loss: 1.279 | Acc: 56.140% (5614/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "0 391 Loss: 1.325 | Acc: 55.469% (71/128)\n",
      "100 391 Loss: 1.237 | Acc: 58.439% (7555/12928)\n",
      "200 391 Loss: 1.232 | Acc: 58.745% (15114/25728)\n",
      "300 391 Loss: 1.231 | Acc: 58.838% (22669/38528)\n",
      "99 100 Loss: 1.275 | Acc: 56.150% (5615/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      "0 391 Loss: 1.074 | Acc: 60.938% (78/128)\n",
      "100 391 Loss: 1.217 | Acc: 59.089% (7639/12928)\n",
      "200 391 Loss: 1.218 | Acc: 59.208% (15233/25728)\n",
      "300 391 Loss: 1.226 | Acc: 58.840% (22670/38528)\n",
      "99 100 Loss: 1.270 | Acc: 56.250% (5625/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      "0 391 Loss: 1.247 | Acc: 59.375% (76/128)\n",
      "100 391 Loss: 1.221 | Acc: 59.143% (7646/12928)\n",
      "200 391 Loss: 1.219 | Acc: 59.274% (15250/25728)\n",
      "300 391 Loss: 1.221 | Acc: 59.154% (22791/38528)\n",
      "99 100 Loss: 1.266 | Acc: 56.320% (5632/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      "0 391 Loss: 1.185 | Acc: 60.938% (78/128)\n",
      "100 391 Loss: 1.220 | Acc: 59.352% (7673/12928)\n",
      "200 391 Loss: 1.215 | Acc: 59.515% (15312/25728)\n",
      "300 391 Loss: 1.218 | Acc: 59.328% (22858/38528)\n",
      "99 100 Loss: 1.263 | Acc: 56.610% (5661/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      "0 391 Loss: 1.240 | Acc: 54.688% (70/128)\n",
      "100 391 Loss: 1.221 | Acc: 59.375% (7676/12928)\n",
      "200 391 Loss: 1.220 | Acc: 59.426% (15289/25728)\n",
      "300 391 Loss: 1.215 | Acc: 59.458% (22908/38528)\n",
      "99 100 Loss: 1.261 | Acc: 56.640% (5664/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 14\n",
      "0 391 Loss: 1.244 | Acc: 56.250% (72/128)\n",
      "100 391 Loss: 1.209 | Acc: 59.916% (7746/12928)\n",
      "200 391 Loss: 1.212 | Acc: 59.682% (15355/25728)\n",
      "300 391 Loss: 1.212 | Acc: 59.562% (22948/38528)\n",
      "99 100 Loss: 1.260 | Acc: 56.710% (5671/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      "0 391 Loss: 1.266 | Acc: 64.062% (82/128)\n",
      "100 391 Loss: 1.215 | Acc: 59.491% (7691/12928)\n",
      "200 391 Loss: 1.217 | Acc: 59.468% (15300/25728)\n",
      "300 391 Loss: 1.212 | Acc: 59.648% (22981/38528)\n",
      "99 100 Loss: 1.258 | Acc: 56.580% (5658/10000)\n",
      "\n",
      "Epoch: 16\n",
      "0 391 Loss: 1.076 | Acc: 69.531% (89/128)\n",
      "100 391 Loss: 1.212 | Acc: 59.862% (7739/12928)\n",
      "200 391 Loss: 1.206 | Acc: 59.748% (15372/25728)\n",
      "300 391 Loss: 1.206 | Acc: 59.764% (23026/38528)\n",
      "99 100 Loss: 1.258 | Acc: 56.580% (5658/10000)\n",
      "\n",
      "Epoch: 17\n",
      "0 391 Loss: 1.062 | Acc: 64.844% (83/128)\n",
      "100 391 Loss: 1.185 | Acc: 60.767% (7856/12928)\n",
      "200 391 Loss: 1.199 | Acc: 60.110% (15465/25728)\n",
      "300 391 Loss: 1.203 | Acc: 59.941% (23094/38528)\n",
      "99 100 Loss: 1.257 | Acc: 56.740% (5674/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 18\n",
      "0 391 Loss: 1.173 | Acc: 60.156% (77/128)\n",
      "100 391 Loss: 1.215 | Acc: 59.491% (7691/12928)\n",
      "200 391 Loss: 1.210 | Acc: 59.565% (15325/25728)\n",
      "300 391 Loss: 1.208 | Acc: 59.731% (23013/38528)\n",
      "99 100 Loss: 1.257 | Acc: 56.850% (5685/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      "0 391 Loss: 1.270 | Acc: 60.156% (77/128)\n",
      "100 391 Loss: 1.216 | Acc: 59.522% (7695/12928)\n",
      "200 391 Loss: 1.211 | Acc: 59.476% (15302/25728)\n",
      "300 391 Loss: 1.207 | Acc: 59.699% (23001/38528)\n",
      "99 100 Loss: 1.256 | Acc: 56.830% (5683/10000)\n",
      "\n",
      "Epoch: 20\n",
      "0 391 Loss: 1.302 | Acc: 53.906% (69/128)\n",
      "100 391 Loss: 1.232 | Acc: 57.874% (7482/12928)\n",
      "200 391 Loss: 1.226 | Acc: 58.151% (14961/25728)\n",
      "300 391 Loss: 1.219 | Acc: 58.539% (22554/38528)\n",
      "99 100 Loss: 1.238 | Acc: 57.370% (5737/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 21\n",
      "0 391 Loss: 1.368 | Acc: 53.906% (69/128)\n",
      "100 391 Loss: 1.177 | Acc: 59.986% (7755/12928)\n",
      "200 391 Loss: 1.171 | Acc: 60.568% (15583/25728)\n",
      "300 391 Loss: 1.171 | Acc: 60.540% (23325/38528)\n",
      "99 100 Loss: 1.193 | Acc: 58.520% (5852/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 22\n",
      "0 391 Loss: 1.110 | Acc: 60.156% (77/128)\n",
      "100 391 Loss: 1.138 | Acc: 62.013% (8017/12928)\n",
      "200 391 Loss: 1.127 | Acc: 62.411% (16057/25728)\n",
      "300 391 Loss: 1.120 | Acc: 62.516% (24086/38528)\n",
      "99 100 Loss: 1.158 | Acc: 59.890% (5989/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      "0 391 Loss: 1.144 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.083 | Acc: 63.668% (8231/12928)\n",
      "200 391 Loss: 1.086 | Acc: 64.066% (16483/25728)\n",
      "300 391 Loss: 1.083 | Acc: 64.065% (24683/38528)\n",
      "99 100 Loss: 1.155 | Acc: 60.420% (6042/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 24\n",
      "0 391 Loss: 0.981 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 1.019 | Acc: 66.839% (8641/12928)\n",
      "200 391 Loss: 1.015 | Acc: 66.989% (17235/25728)\n",
      "300 391 Loss: 1.025 | Acc: 66.591% (25656/38528)\n",
      "99 100 Loss: 1.106 | Acc: 62.530% (6253/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 25\n",
      "0 391 Loss: 0.858 | Acc: 72.656% (93/128)\n",
      "100 391 Loss: 1.017 | Acc: 67.218% (8690/12928)\n",
      "200 391 Loss: 1.011 | Acc: 67.374% (17334/25728)\n",
      "300 391 Loss: 1.010 | Acc: 67.239% (25906/38528)\n",
      "99 100 Loss: 1.097 | Acc: 62.950% (6295/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 26\n",
      "0 391 Loss: 0.983 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 0.994 | Acc: 67.713% (8754/12928)\n",
      "200 391 Loss: 1.004 | Acc: 67.386% (17337/25728)\n",
      "300 391 Loss: 1.003 | Acc: 67.374% (25958/38528)\n",
      "99 100 Loss: 1.087 | Acc: 63.130% (6313/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 27\n",
      "0 391 Loss: 1.031 | Acc: 67.969% (87/128)\n",
      "100 391 Loss: 0.981 | Acc: 68.773% (8891/12928)\n",
      "200 391 Loss: 0.991 | Acc: 68.381% (17593/25728)\n",
      "300 391 Loss: 0.987 | Acc: 68.348% (26333/38528)\n",
      "99 100 Loss: 1.082 | Acc: 63.250% (6325/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 28\n",
      "0 391 Loss: 0.881 | Acc: 73.438% (94/128)\n",
      "100 391 Loss: 0.974 | Acc: 68.441% (8848/12928)\n",
      "200 391 Loss: 0.969 | Acc: 69.057% (17767/25728)\n",
      "300 391 Loss: 0.971 | Acc: 69.028% (26595/38528)\n",
      "99 100 Loss: 1.070 | Acc: 63.730% (6373/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 29\n",
      "0 391 Loss: 0.906 | Acc: 69.531% (89/128)\n",
      "100 391 Loss: 0.977 | Acc: 68.773% (8891/12928)\n",
      "200 391 Loss: 0.966 | Acc: 69.076% (17772/25728)\n",
      "300 391 Loss: 0.965 | Acc: 69.248% (26680/38528)\n",
      "99 100 Loss: 1.067 | Acc: 63.710% (6371/10000)\n",
      "\n",
      "Epoch: 30\n",
      "0 391 Loss: 1.090 | Acc: 65.625% (84/128)\n",
      "100 391 Loss: 0.956 | Acc: 69.377% (8969/12928)\n",
      "200 391 Loss: 0.963 | Acc: 69.294% (17828/25728)\n",
      "300 391 Loss: 0.960 | Acc: 69.547% (26795/38528)\n",
      "99 100 Loss: 1.065 | Acc: 64.190% (6419/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 31\n",
      "0 391 Loss: 0.809 | Acc: 76.562% (98/128)\n",
      "100 391 Loss: 0.957 | Acc: 69.787% (9022/12928)\n",
      "200 391 Loss: 0.953 | Acc: 69.640% (17917/25728)\n",
      "300 391 Loss: 0.956 | Acc: 69.604% (26817/38528)\n",
      "99 100 Loss: 1.064 | Acc: 64.100% (6410/10000)\n",
      "\n",
      "Epoch: 32\n",
      "0 391 Loss: 1.031 | Acc: 63.281% (81/128)\n",
      "100 391 Loss: 0.953 | Acc: 69.524% (8988/12928)\n",
      "200 391 Loss: 0.950 | Acc: 69.776% (17952/25728)\n",
      "300 391 Loss: 0.950 | Acc: 69.887% (26926/38528)\n",
      "99 100 Loss: 1.059 | Acc: 64.010% (6401/10000)\n",
      "\n",
      "Epoch: 33\n",
      "0 391 Loss: 0.930 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 0.943 | Acc: 70.359% (9096/12928)\n",
      "200 391 Loss: 0.948 | Acc: 70.083% (18031/25728)\n",
      "300 391 Loss: 0.950 | Acc: 69.983% (26963/38528)\n",
      "99 100 Loss: 1.058 | Acc: 64.030% (6403/10000)\n",
      "\n",
      "Epoch: 34\n",
      "0 391 Loss: 0.918 | Acc: 74.219% (95/128)\n",
      "100 391 Loss: 0.958 | Acc: 70.173% (9072/12928)\n",
      "200 391 Loss: 0.955 | Acc: 69.998% (18009/25728)\n",
      "300 391 Loss: 0.950 | Acc: 70.172% (27036/38528)\n",
      "99 100 Loss: 1.058 | Acc: 64.390% (6439/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 35\n",
      "0 391 Loss: 0.912 | Acc: 73.438% (94/128)\n",
      "100 391 Loss: 0.944 | Acc: 70.452% (9108/12928)\n",
      "200 391 Loss: 0.943 | Acc: 70.379% (18107/25728)\n",
      "300 391 Loss: 0.947 | Acc: 70.261% (27070/38528)\n",
      "99 100 Loss: 1.058 | Acc: 64.370% (6437/10000)\n",
      "\n",
      "Epoch: 36\n",
      "0 391 Loss: 0.872 | Acc: 74.219% (95/128)\n",
      "100 391 Loss: 0.939 | Acc: 70.320% (9091/12928)\n",
      "200 391 Loss: 0.945 | Acc: 70.141% (18046/25728)\n",
      "300 391 Loss: 0.948 | Acc: 70.040% (26985/38528)\n",
      "99 100 Loss: 1.057 | Acc: 64.280% (6428/10000)\n",
      "\n",
      "Epoch: 37\n",
      "0 391 Loss: 0.943 | Acc: 74.219% (95/128)\n",
      "100 391 Loss: 0.933 | Acc: 70.289% (9087/12928)\n",
      "200 391 Loss: 0.945 | Acc: 70.173% (18054/25728)\n",
      "300 391 Loss: 0.945 | Acc: 70.222% (27055/38528)\n",
      "99 100 Loss: 1.056 | Acc: 64.310% (6431/10000)\n",
      "\n",
      "Epoch: 38\n",
      "0 391 Loss: 0.951 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 0.950 | Acc: 69.988% (9048/12928)\n",
      "200 391 Loss: 0.949 | Acc: 70.208% (18063/25728)\n",
      "300 391 Loss: 0.944 | Acc: 70.338% (27100/38528)\n",
      "99 100 Loss: 1.056 | Acc: 64.400% (6440/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 39\n",
      "0 391 Loss: 0.854 | Acc: 71.875% (92/128)\n",
      "100 391 Loss: 0.940 | Acc: 69.957% (9044/12928)\n",
      "200 391 Loss: 0.944 | Acc: 70.188% (18058/25728)\n",
      "300 391 Loss: 0.945 | Acc: 70.175% (27037/38528)\n",
      "99 100 Loss: 1.056 | Acc: 64.490% (6449/10000)\n",
      "Saving..\n",
      "64.49\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a dataset, try loading it and doing standard training\n",
    "# Uses a new dataset, requires knowing the epsilon, p, rho that were used to produce it (these are in the file name)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from cifar10_models import return_model\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "\n",
    "from numpy import load\n",
    "import numpy as np\n",
    "\n",
    "name = 'fclinearl1'\n",
    "name = 'convlinearl1k32c3'\n",
    "\n",
    "#name = 'convlinearl1k3c3'\n",
    "#name = 'convlinearl1k3c32'\n",
    "\n",
    "# For contaminated data, Josue wants linear vs nonlinear, FC vs Conv\n",
    "# Linear FC : 'linear'\n",
    "# Nonlinear FC : 'fclinear'\n",
    "# Linear Conv : 'simplelinear'\n",
    "# Nonlinear Conv : 'simple'\n",
    "\n",
    "name = 'linear'\n",
    "name = 'fclinear'\n",
    "name = 'simplelinear'\n",
    "name = 'simple'\n",
    "\n",
    "args = {}\n",
    "args['resume'] = False\n",
    "args['lr'] = 0.1\n",
    "args['lr']/=50\n",
    "# .1 too fast for fclinearl1\n",
    "# fclinearl1 /10 : 41.5%\n",
    "# convlinearl1k32c3 /50 42%\n",
    "# fclinearl1 v2 /10 42%\n",
    "# convlinearl1k32c3 v2 /50 42%\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "\n",
    "class CIFAR10Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"CIFAR Dataset.\"\"\"\n",
    "    def __init__(self, dataset_path='./data/modified_cifar_test_eps:{}_p:{}_rho:{}.npy.npz'.format(3e-5,1,0), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.file = np.load(dataset_path, allow_pickle=True)\n",
    "        self.images = self.file['images'].astype(np.single)\n",
    "        self.target = self.file['target']\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        target = self.target[idx]\n",
    "        if self.transform !=None:\n",
    "            img = self.transform(img)\n",
    "        img = torch.from_numpy(img)\n",
    "        return img, target\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "epsilon = 5e-3 * 50 # random perturbation size - Ankit requested that it be 'comparable' to L1 norm of perturbations. These tend to be around .03, so choose .03/(32*32*p)? ~= 3e-5\n",
    "p = .2 # random perturbation density from 1 for full to e.g. .01 for very sparse\n",
    "rho = 0\n",
    "\n",
    "#trainset = torchvision.datasets.CIFAR10(\n",
    "#    root='./data', train=True, download=True, transform=transform_train)\n",
    "#trainset = CIFAR10Dataset(dataset_path = './data/modified_cifar_train_eps:{}_p:{}_rho:{}.npy.npz'.format(3e-5,1,0))#, transform = transform_train)\n",
    "#trainset = CIFAR10Dataset(dataset_path = './data/modified_cifar_train_eps:{}_p:{}_rho:{}.npy.npz'.format(0.0015,0.02,0))#, transform = transform_train)\n",
    "trainset = CIFAR10Dataset(dataset_path = './data/modified_cifar_train_eps:{}_p:{}_rho:{}.npy.npz'.format(epsilon,p,rho))#, transform = transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=1)\n",
    "\n",
    "#testset = torchvision.datasets.CIFAR10(\n",
    "#    root='./data', train=False, download=True, transform=transform_test)\n",
    "#testset = CIFAR10Dataset(dataset_path = './data/modified_cifar_test_eps:{}_p:{}_rho:{}.npy.npz'.format(3e-5,1,0))#, transform = transform_test)\n",
    "#testset = CIFAR10Dataset(dataset_path = './data/modified_cifar_test_eps:{}_p:{}_rho:{}.npy.npz'.format(0.0015,0.02,0))#, transform = transform_test)\n",
    "testset = CIFAR10Dataset(dataset_path = './data/modified_cifar_test_eps:{}_p:{}_rho:{}.npy.npz'.format(epsilon,p,rho))#, transform = transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=1)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "#print(trainset.__getitem__(0))\n",
    "\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = return_model(name)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if args['resume']:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args['lr'],\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Change name - based on contamination\n",
    "# Base (p = 1) - no change\n",
    "# Sparse (p = .02) : v2\n",
    "# Less sparse, stronger : v3\n",
    "\n",
    "name +='v3'\n",
    "\n",
    "\n",
    "lambda1 = lambda epoch: 0.3**((epoch%20)//4)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "\n",
    "def train(epoch,name):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        targets = targets.type(torch.long)\n",
    "        #print(inputs.shape)\n",
    "        #if epoch == 0 and batch_idx == 0:\n",
    "        if np.mod(batch_idx,30)==1:\n",
    "            torch.save(net.state_dict(), './cifar_resnet/ckpt_{}_{}_{}.pth'.format(name, epoch, batch_idx))\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        #if batch_idx%1 == 0:\n",
    "            #torch.save(net.state_dict(), './cifar_resnet/ckpt_track_{}_{}.pth'.format(epoch, batch_idx+1))\n",
    "        if batch_idx%100 == 0:\n",
    "            print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'%(train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch,name):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            targets = targets.type(torch.long)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'% (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('cifar_resnet'):\n",
    "            os.mkdir('cifar_resnet')\n",
    "        torch.save(state, './cifar_resnet/ckpt_{}.pth'.format(name))\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+40):\n",
    "    train(epoch,name)\n",
    "    test(epoch,name)\n",
    "    scheduler.step()\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 Poison - no real difference from baseline\n",
    "# Poisoned L FC - 40.86, 40.74, 40.81, 40.81, 40.8\n",
    "# Poisoned NL FC - 48.49, 48.47, 48.59, 48.45, 48.49\n",
    "# Poisoned L C - 41.94, 42.03, 41.86, 42.02, 41.97\n",
    "# Poisoned NL C - 57.02, 57.14, 57.55, 56.94, 56.8\n",
    "# V3 Poison -  (.25/.2)\n",
    "# L FC   : 46.15, 46.06, 46.03, 46.05, 46.23\n",
    "# NL FC  : 51.89, 51.44, 51.89, 52.08, 52.01\n",
    "# L Conv : 73.95, 73.16, 74.05, 74.48, 74.98\n",
    "# NL Conv: 65.2, 67.89, 65.74, 66.85, 65.9 / 67.14, 65.39, 66.61, 65.46, 64.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32])\n",
      "(10000, 3, 32, 32)\n",
      "(10000, 3, 32, 32)\n",
      "(3, 32, 32)\n",
      "torch.Size([32, 3, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "t = trainset.__getitem__(0)\n",
    "print(t[0].shape) #???\n",
    "testfile = np.load('./data/modified_cifar_test_eps:{}_p:{}_rho:{}.npy.npz'.format(3e-5,1,0), allow_pickle=True)\n",
    "testimages = testfile['images']#.astype(np.single)\n",
    "print(testimages.shape)\n",
    "testimages = testimages.astype(np.single)\n",
    "print(testimages.shape)\n",
    "print(testimages[0].shape)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "testimagesv2 = transform_train(testimages[0])\n",
    "print(testimagesv2.shape)\n",
    "print(torch.from_numpy(testimages[0]).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
